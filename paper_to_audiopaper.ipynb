{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai python-dotenv backoff arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Cython==0.29.21\n",
    "!pip install librosa==0.8.0\n",
    "!pip install phonemizer==2.2.1\n",
    "!pip install scipy\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install matplotlib\n",
    "!pip install Unidecode==1.1.1\n",
    "!pip install python3-commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "!git clone https://github.com/jaywalnut310/vits.git\n",
    "!python --version\n",
    "%cd vits/\n",
    "\n",
    "\n",
    "# Fix numpy deprecation by removing \"np.\"\"\n",
    "%cd monotonic_align/\n",
    "%mkdir monotonic_align\n",
    "!python setup.py build_ext --inplace\n",
    "%cd ../\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd vits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import locale\n",
    "\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "\n",
    "def download(lang, tgt_dir=\"./\"):\n",
    "    lang_fn, lang_dir = os.path.join(tgt_dir, lang + \".tar.gz\"), os.path.join(\n",
    "        tgt_dir, lang\n",
    "    )\n",
    "    cmd = \";\".join(\n",
    "        [\n",
    "            f\"wget https://dl.fbaipublicfiles.com/mms/tts/{lang}.tar.gz -O {lang_fn}\",\n",
    "            f\"tar zxvf {lang_fn}\",\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Download model for language: {lang}\")\n",
    "    subprocess.check_output(cmd, shell=True)\n",
    "    print(f\"Model checkpoints in {lang_dir}: {os.listdir(lang_dir)}\")\n",
    "    return lang_dir\n",
    "\n",
    "\n",
    "LANG = \"eng\"\n",
    "ckpt_dir = download(LANG)\n",
    "\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import tempfile\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import commons\n",
    "#import utils\n",
    "import argparse\n",
    "import subprocess\n",
    "from data_utils import (\n",
    "    TextAudioLoader,\n",
    "    TextAudioCollate,\n",
    "    TextAudioSpeakerLoader,\n",
    "    TextAudioSpeakerCollate,\n",
    ")\n",
    "from models import SynthesizerTrn\n",
    "from scipy.io.wavfile import write\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "def preprocess_char(text, lang=None):\n",
    "    \"\"\"\n",
    "    Special treatement of characters in certain languages\n",
    "    \"\"\"\n",
    "    print(lang)\n",
    "    if lang == \"ron\":\n",
    "        text = text.replace(\"ț\", \"ţ\")\n",
    "    return text\n",
    "\n",
    "\n",
    "class TextMapper(object):\n",
    "    def __init__(self, vocab_file):\n",
    "        self.symbols = [\n",
    "            x.replace(\"\\n\", \"\") for x in open(vocab_file, encoding=\"utf-8\").readlines()\n",
    "        ]\n",
    "        self.SPACE_ID = self.symbols.index(\" \")\n",
    "        self._symbol_to_id = {s: i for i, s in enumerate(self.symbols)}\n",
    "        self._id_to_symbol = {i: s for i, s in enumerate(self.symbols)}\n",
    "\n",
    "    def text_to_sequence(self, text, cleaner_names):\n",
    "        \"\"\"Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "        Args:\n",
    "        text: string to convert to a sequence\n",
    "        cleaner_names: names of the cleaner functions to run the text through\n",
    "        Returns:\n",
    "        List of integers corresponding to the symbols in the text\n",
    "        \"\"\"\n",
    "        sequence = []\n",
    "        clean_text = text.strip()\n",
    "        for symbol in clean_text:\n",
    "            symbol_id = self._symbol_to_id[symbol]\n",
    "            sequence += [symbol_id]\n",
    "        return sequence\n",
    "\n",
    "    def uromanize(self, text, uroman_pl):\n",
    "        iso = \"xxx\"\n",
    "        with tempfile.NamedTemporaryFile() as tf, tempfile.NamedTemporaryFile() as tf2:\n",
    "            with open(tf.name, \"w\") as f:\n",
    "                f.write(\"\\n\".join([text]))\n",
    "            cmd = f\"perl \" + uroman_pl\n",
    "            cmd += f\" -l {iso} \"\n",
    "            cmd += f\" < {tf.name} > {tf2.name}\"\n",
    "            os.system(cmd)\n",
    "            outtexts = []\n",
    "            with open(tf2.name) as f:\n",
    "                for line in f:\n",
    "                    line = re.sub(r\"\\s+\", \" \", line).strip()\n",
    "                    outtexts.append(line)\n",
    "            outtext = outtexts[0]\n",
    "        return outtext\n",
    "\n",
    "    def get_text(self, text, hps):\n",
    "        text_norm = self.text_to_sequence(text, hps.data.text_cleaners)\n",
    "        if hps.data.add_blank:\n",
    "            text_norm = commons.intersperse(text_norm, 0)\n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def filter_oov(self, text):\n",
    "        val_chars = self._symbol_to_id\n",
    "        txt_filt = \"\".join(list(filter(lambda x: x in val_chars, text)))\n",
    "        print(f\"text after filtering OOV: {txt_filt}\")\n",
    "        return txt_filt\n",
    "\n",
    "\n",
    "def preprocess_text(txt, text_mapper, hps, uroman_dir=None, lang=None):\n",
    "    txt = preprocess_char(txt, lang=lang)\n",
    "    is_uroman = hps.data.training_files.split(\".\")[-1] == \"uroman\"\n",
    "    if is_uroman:\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            if uroman_dir is None:\n",
    "                cmd = f\"git clone git@github.com:isi-nlp/uroman.git {tmp_dir}\"\n",
    "                print(cmd)\n",
    "                subprocess.check_output(cmd, shell=True)\n",
    "                uroman_dir = tmp_dir\n",
    "            uroman_pl = os.path.join(uroman_dir, \"bin\", \"uroman.pl\")\n",
    "            print(f\"uromanize\")\n",
    "            txt = text_mapper.uromanize(txt, uroman_pl)\n",
    "            print(f\"uroman text: {txt}\")\n",
    "    txt = txt.lower()\n",
    "    txt = text_mapper.filter_oov(txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Run inference with {device}\")\n",
    "vocab_file = f\"{ckpt_dir}/vocab.txt\"\n",
    "config_file = f\"{ckpt_dir}/config.json\"\n",
    "assert os.path.isfile(config_file), f\"{config_file} doesn't exist\"\n",
    "import utils\n",
    "hps = utils.get_hparams_from_file(config_file)\n",
    "text_mapper = TextMapper(vocab_file)\n",
    "net_g = SynthesizerTrn(\n",
    "    len(text_mapper.symbols),\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    **hps.model,\n",
    ")\n",
    "net_g.to(device)\n",
    "_ = net_g.eval()\n",
    "\n",
    "g_pth = f\"{ckpt_dir}/G_100000.pth\"\n",
    "print(f\"load {g_pth}\")\n",
    "\n",
    "_ = utils.load_checkpoint(g_pth, net_g, None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import json\n",
    "import concurrent.futures\n",
    "import backoff\n",
    "import time\n",
    "from scipy.io.wavfile import write\n",
    "import os\n",
    "import subprocess\n",
    "import arxiv\n",
    "from typing import Union\n",
    "from typing import List, Dict\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "MAX_WORKERS = 10  # Maximum number of parallel tasks\n",
    "MAX_TRIES = 10  # Maximum number of tries for a single request\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pdf_to_markdown(input: Union[str, bool], output_dir: str = \"outputs\", is_arxiv_id: bool = False, nougat_path: str = \"/home/iwe30/anaconda3/envs/nougat/bin\"):\n",
    "    \"\"\"\n",
    "    Converts a PDF file or an arXiv paper to a markdown file using nougat.\n",
    "\n",
    "    Args:\n",
    "        input (Union[str, bool]): Path to the PDF file or an arXiv ID.\n",
    "        output_dir (str, optional): Path to the output directory. Defaults to \"outputs\".\n",
    "        is_arxiv_id (bool, optional): If True, the input is treated as an arXiv ID. Defaults to False.\n",
    "        nougat_path (str, optional): Path to the nougat executable. Defaults to \"/home/iwe30/anaconda3/envs/nougat/bin\".\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input is neither a valid PDF file nor a valid arXiv ID.\n",
    "    \"\"\"\n",
    "    # Check if the output directory exists, if not create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # If the input is an Arxiv ID, download the paper first\n",
    "    if is_arxiv_id:\n",
    "        paper = next(arxiv.Search(id_list=[input]).results())\n",
    "        pdf_path = f\"{output_dir}/{input}/{input}.pdf\"\n",
    "        # Create a directory for the output\n",
    "        markdown_dir = f\"{output_dir}/{input}\"\n",
    "        if not os.path.exists(markdown_dir):\n",
    "            os.makedirs(markdown_dir)\n",
    "        paper.download_pdf(dirpath=f\"{output_dir}/{input}\", filename=f\"{input}.pdf\")\n",
    "    else:\n",
    "        pdf_path = input\n",
    "        input = os.path.splitext(os.path.basename(input))[0]\n",
    "\n",
    "    # Run the nougat command\n",
    "    cmd = f\"{nougat_path}/nougat {pdf_path} -o {markdown_dir} --markdown\"\n",
    "    subprocess.run(cmd, shell=True)\n",
    "    \n",
    "\n",
    "@backoff.on_exception(backoff.expo, (Exception,), max_tries=MAX_TRIES)\n",
    "def translate(latex_formula: str) -> str:\n",
    "    \"\"\"\n",
    "    Translate a LaTeX formula to plain English using OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        latex_formula (str): The LaTeX formula to be translated.\n",
    "\n",
    "    Returns:\n",
    "        str: The translation of the LaTeX formula in plain English.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the translation request fails, the function will retry for a maximum of MAX_TRIES times.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. If the formula is just a letter keep the letter or letters. examples: \\(C_{\\text{gen}}\\)  = C sub gen.  you always reply with the translation nothing else!!!\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Read this latex formula in plain english :  {latex_formula}\"},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Translated LaTeX: {latex_formula} to English: {response['choices'][0]['message']['content']}\")\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "def latex_to_english(latex_formulas: List[str], debug: bool = False) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Translate a list of LaTeX formulas to plain English using OpenAI API.\n",
    "\n",
    "    :param latex_formulas: list, a list of LaTeX formulas\n",
    "    :param debug: bool, optional, if True, print debug information. Defaults to False.\n",
    "    \n",
    "    :return: dict, a dictionary where key is the LaTeX formula and value is the translated English text\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        logging.basicConfig(level=logging.DEBUG)\n",
    "    else:\n",
    "        pass\n",
    "        #logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    translated_formulas = {}\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_formula = {executor.submit(translate, formula): formula for formula in latex_formulas}\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_formula):\n",
    "            formula = future_to_formula[future]\n",
    "            try:\n",
    "                translation = future.result()\n",
    "                translated_formulas[formula] = translation\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"{formula} generated an exception: {exc}\")\n",
    "\n",
    "    return translated_formulas\n",
    "\n",
    "def process_markdown_file(file_path: str, dry_run: bool = False, debug: bool = False, tag: str = \"_processed\") -> None:\n",
    "    \"\"\"\n",
    "    Process a markdown file, translating LaTeX formulas to plain English.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the markdown file.\n",
    "        dry_run (bool, optional): If True, the function doesn't write changes back to the file. Defaults to False.\n",
    "        debug (bool, optional): If True, the function prints debug information. Defaults to False.\n",
    "        tag (str, optional): Tag to append to the filename of the processed file. Defaults to \"_processed\".\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file specified by `file_path` does not exist.\n",
    "        ValueError: If `file_path` is not a valid markdown file (i.e., does not have a .md or .markdown extension).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "    # Check if the file is a markdown file\n",
    "    if not file_path.endswith(('.md', '.markdown')):\n",
    "        raise ValueError(f\"The file {file_path} is not a valid markdown file.\")\n",
    "\n",
    "    # Dictionary to store LaTeX formulas\n",
    "    latex_formulas = {}\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        matches = re.findall(r\"\\\\\\[(.*?)\\\\\\]|\\\\\\((.*?)\\\\\\)\", line)  # find both inline and display LaTeX formulas\n",
    "        for match in matches:\n",
    "            match = [x for x in match if x]  # remove empty string \n",
    "            if match:\n",
    "                formula = match[0]  # result is in a list\n",
    "                if debug:\n",
    "                    print(f\"Found LaTeX formula: {formula}\")\n",
    "                if formula not in latex_formulas:\n",
    "                    latex_formulas[formula] = None  # Add new formula to the dictionary\n",
    "\n",
    "    print(f\"We found {len(latex_formulas)} formulas in your document\")\n",
    "\n",
    "    # Translate all LaTeX formulas\n",
    "    latex_formulas = latex_to_english(list(latex_formulas.keys()), debug)\n",
    "\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        for formula, translation in latex_formulas.items():\n",
    "            # Replace the formula with its translation\n",
    "            line = line.replace(f\"\\\\[{formula}\\\\]\", translation)\n",
    "            line = line.replace(f\"\\\\({formula}\\\\)\", translation)\n",
    "\n",
    "        new_lines.append(line)\n",
    "\n",
    "    if not dry_run:\n",
    "        base, ext = os.path.splitext(file_path)\n",
    "        new_file_path = f\"{base}{tag}{ext}\"\n",
    "        with open(new_file_path, \"w\", encoding=\"utf8\") as f:\n",
    "            f.writelines(new_lines)\n",
    "\n",
    "def markdown_to_wav(file_path, output_path='output.wav'):\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    audio_result = []\n",
    "    for line in lines:\n",
    "        # Preprocess the text\n",
    "        text = preprocess_text(line, text_mapper, hps, lang=LANG)\n",
    "\n",
    "        # Convert text to tensor\n",
    "        stn_tst = text_mapper.get_text(text, hps)\n",
    "        \n",
    "        # Convert tensor to PyTorch tensor and perform inference\n",
    "        with torch.no_grad():\n",
    "            x_tst = stn_tst.unsqueeze(0).to(device)\n",
    "            x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "            hyp = net_g.infer(\n",
    "                x_tst, x_tst_lengths, noise_scale=.667,\n",
    "                noise_scale_w=0.8, length_scale=1.0\n",
    "            )[0][0,0].cpu().float().numpy()\n",
    "        \n",
    "        audio_result.append(hyp)\n",
    "    \n",
    "    audio_result = np.concatenate(audio_result)\n",
    "\n",
    "    # Save the audio to a WAV file\n",
    "    write(output_path, hps.data.sampling_rate, audio_result)\n",
    "\n",
    "    print(f\"Generated audio: {output_path}\") \n",
    "    return Audio(audio_result, rate=hps.data.sampling_rate)\n",
    "\n",
    "def pdf_to_speech(pdf_input, output_dir=\"outputs\", is_arxiv_id=False, dry_run=False, debug=False, wav_output_path='output.wav'):\n",
    "    \"\"\"\n",
    "    Convert a PDF file (or an ArXiv ID) to speech.\n",
    "\n",
    "    :param pdf_input: str, path to the PDF file or an ArXiv ID\n",
    "    :param output_dir: str, path to the output directory\n",
    "    :param is_arxiv_id: bool, if True, pdf_input is treated as an ArXiv ID\n",
    "    :param dry_run: bool, if True, don't write changes back to the file\n",
    "    :param debug: bool, if True, print debug information\n",
    "    :param wav_output_path: str, path to the output WAV file\n",
    "    \"\"\"\n",
    "    # Step 1: Convert PDF to markdown\n",
    "    pdf_to_markdown(pdf_input, output_dir=output_dir, is_arxiv_id=is_arxiv_id)\n",
    "    \n",
    "    # Determine the markdown file path\n",
    "    if is_arxiv_id:\n",
    "        input_name = pdf_input\n",
    "    else:\n",
    "        input_name = os.path.splitext(os.path.basename(pdf_input))[0]\n",
    "    markdown_file_path = f\"{output_dir}/{input_name}/{input_name}.mmd\"\n",
    "    \n",
    "    # Step 2: Process the markdown file\n",
    "    process_markdown_file(markdown_file_path, dry_run=dry_run, debug=debug)\n",
    "    \n",
    "    # Determine the processed markdown file path\n",
    "    processed_markdown_file_path = f\"{output_dir}/{input_name}/{input_name}_processed.mmd\"\n",
    "    \n",
    "    # Step 3: Convert markdown to speech\n",
    "    wav_output_path = f\"{output_dir}/{input_name}/{input_name}.wav\"\n",
    "    markdown_to_wav(processed_markdown_file_path, output_path=wav_output_path)\n",
    "    \n",
    "    print(f\"Process completed. The speech audio is saved as {wav_output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arxiv to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_to_speech(\"2309.07124\", is_arxiv_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
