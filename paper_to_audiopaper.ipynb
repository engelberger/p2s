{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai python-dotenv backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "!git clone https://github.com/jaywalnut310/vits.git\n",
    "!python --version\n",
    "%cd vits/\n",
    "\n",
    "!pip install Cython==0.29.21\n",
    "!pip install librosa==0.8.0\n",
    "!pip install phonemizer==2.2.1\n",
    "!pip install scipy\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install matplotlib\n",
    "!pip install Unidecode==1.1.1\n",
    "!pip install python3-commons\n",
    "# Fix numpy deprecation by removing \"np.\"\"\n",
    "%cd monotonic_align/\n",
    "%mkdir monotonic_align\n",
    "!python setup.py build_ext --inplace\n",
    "%cd ../\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import locale\n",
    "\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "\n",
    "def download(lang, tgt_dir=\"./\"):\n",
    "    lang_fn, lang_dir = os.path.join(tgt_dir, lang + \".tar.gz\"), os.path.join(\n",
    "        tgt_dir, lang\n",
    "    )\n",
    "    cmd = \";\".join(\n",
    "        [\n",
    "            f\"wget https://dl.fbaipublicfiles.com/mms/tts/{lang}.tar.gz -O {lang_fn}\",\n",
    "            f\"tar zxvf {lang_fn}\",\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Download model for language: {lang}\")\n",
    "    subprocess.check_output(cmd, shell=True)\n",
    "    print(f\"Model checkpoints in {lang_dir}: {os.listdir(lang_dir)}\")\n",
    "    return lang_dir\n",
    "\n",
    "\n",
    "LANG = \"eng\"\n",
    "#ckpt_dir = download(LANG)\n",
    "\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import tempfile\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import commons\n",
    "#import utils\n",
    "import argparse\n",
    "import subprocess\n",
    "from data_utils import (\n",
    "    TextAudioLoader,\n",
    "    TextAudioCollate,\n",
    "    TextAudioSpeakerLoader,\n",
    "    TextAudioSpeakerCollate,\n",
    ")\n",
    "from models import SynthesizerTrn\n",
    "from scipy.io.wavfile import write\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "def preprocess_char(text, lang=None):\n",
    "    \"\"\"\n",
    "    Special treatement of characters in certain languages\n",
    "    \"\"\"\n",
    "    print(lang)\n",
    "    if lang == \"ron\":\n",
    "        text = text.replace(\"ț\", \"ţ\")\n",
    "    return text\n",
    "\n",
    "\n",
    "class TextMapper(object):\n",
    "    def __init__(self, vocab_file):\n",
    "        self.symbols = [\n",
    "            x.replace(\"\\n\", \"\") for x in open(vocab_file, encoding=\"utf-8\").readlines()\n",
    "        ]\n",
    "        self.SPACE_ID = self.symbols.index(\" \")\n",
    "        self._symbol_to_id = {s: i for i, s in enumerate(self.symbols)}\n",
    "        self._id_to_symbol = {i: s for i, s in enumerate(self.symbols)}\n",
    "\n",
    "    def text_to_sequence(self, text, cleaner_names):\n",
    "        \"\"\"Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "        Args:\n",
    "        text: string to convert to a sequence\n",
    "        cleaner_names: names of the cleaner functions to run the text through\n",
    "        Returns:\n",
    "        List of integers corresponding to the symbols in the text\n",
    "        \"\"\"\n",
    "        sequence = []\n",
    "        clean_text = text.strip()\n",
    "        for symbol in clean_text:\n",
    "            symbol_id = self._symbol_to_id[symbol]\n",
    "            sequence += [symbol_id]\n",
    "        return sequence\n",
    "\n",
    "    def uromanize(self, text, uroman_pl):\n",
    "        iso = \"xxx\"\n",
    "        with tempfile.NamedTemporaryFile() as tf, tempfile.NamedTemporaryFile() as tf2:\n",
    "            with open(tf.name, \"w\") as f:\n",
    "                f.write(\"\\n\".join([text]))\n",
    "            cmd = f\"perl \" + uroman_pl\n",
    "            cmd += f\" -l {iso} \"\n",
    "            cmd += f\" < {tf.name} > {tf2.name}\"\n",
    "            os.system(cmd)\n",
    "            outtexts = []\n",
    "            with open(tf2.name) as f:\n",
    "                for line in f:\n",
    "                    line = re.sub(r\"\\s+\", \" \", line).strip()\n",
    "                    outtexts.append(line)\n",
    "            outtext = outtexts[0]\n",
    "        return outtext\n",
    "\n",
    "    def get_text(self, text, hps):\n",
    "        text_norm = self.text_to_sequence(text, hps.data.text_cleaners)\n",
    "        if hps.data.add_blank:\n",
    "            text_norm = commons.intersperse(text_norm, 0)\n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def filter_oov(self, text):\n",
    "        val_chars = self._symbol_to_id\n",
    "        txt_filt = \"\".join(list(filter(lambda x: x in val_chars, text)))\n",
    "        print(f\"text after filtering OOV: {txt_filt}\")\n",
    "        return txt_filt\n",
    "\n",
    "\n",
    "def preprocess_text(txt, text_mapper, hps, uroman_dir=None, lang=None):\n",
    "    txt = preprocess_char(txt, lang=lang)\n",
    "    is_uroman = hps.data.training_files.split(\".\")[-1] == \"uroman\"\n",
    "    if is_uroman:\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            if uroman_dir is None:\n",
    "                cmd = f\"git clone git@github.com:isi-nlp/uroman.git {tmp_dir}\"\n",
    "                print(cmd)\n",
    "                subprocess.check_output(cmd, shell=True)\n",
    "                uroman_dir = tmp_dir\n",
    "            uroman_pl = os.path.join(uroman_dir, \"bin\", \"uroman.pl\")\n",
    "            print(f\"uromanize\")\n",
    "            txt = text_mapper.uromanize(txt, uroman_pl)\n",
    "            print(f\"uroman text: {txt}\")\n",
    "    txt = txt.lower()\n",
    "    txt = text_mapper.filter_oov(txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Run inference with {device}\")\n",
    "vocab_file = f\"{ckpt_dir}/vocab.txt\"\n",
    "config_file = f\"{ckpt_dir}/config.json\"\n",
    "assert os.path.isfile(config_file), f\"{config_file} doesn't exist\"\n",
    "import utils\n",
    "hps = utils.get_hparams_from_file(config_file)\n",
    "text_mapper = TextMapper(vocab_file)\n",
    "net_g = SynthesizerTrn(\n",
    "    len(text_mapper.symbols),\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    **hps.model,\n",
    ")\n",
    "net_g.to(device)\n",
    "_ = net_g.eval()\n",
    "\n",
    "g_pth = f\"{ckpt_dir}/G_100000.pth\"\n",
    "print(f\"load {g_pth}\")\n",
    "\n",
    "_ = utils.load_checkpoint(g_pth, net_g, None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def markdown_to_wav(file_path, output_path='output.wav'):\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    audio_result = []\n",
    "    for line in lines:\n",
    "        # Preprocess the text\n",
    "        text = preprocess_text(line, text_mapper, hps, lang=LANG)\n",
    "\n",
    "        # Convert text to tensor\n",
    "        stn_tst = text_mapper.get_text(text, hps)\n",
    "        \n",
    "        # Convert tensor to PyTorch tensor and perform inference\n",
    "        with torch.no_grad():\n",
    "            x_tst = stn_tst.unsqueeze(0).to(device)\n",
    "            x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "            hyp = net_g.infer(\n",
    "                x_tst, x_tst_lengths, noise_scale=.667,\n",
    "                noise_scale_w=0.8, length_scale=1.0\n",
    "            )[0][0,0].cpu().float().numpy()\n",
    "        \n",
    "        audio_result.append(hyp)\n",
    "    \n",
    "    audio_result = np.concatenate(audio_result)\n",
    "\n",
    "    # Save the audio to a WAV file\n",
    "    write(output_path, hps.data.sampling_rate, audio_result)\n",
    "\n",
    "    print(f\"Generated audio: {output_path}\") \n",
    "    return Audio(audio_result, rate=hps.data.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import json\n",
    "import concurrent.futures\n",
    "import backoff\n",
    "import time\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "MAX_WORKERS = 10  # Maximum number of parallel tasks\n",
    "MAX_TRIES = 10  # Maximum number of tries for a single request\n",
    "\n",
    "@backoff.on_exception(backoff.expo, (Exception,), max_tries=MAX_TRIES)\n",
    "def translate(latex_formula):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. If the formula is just a letter keep the letter or letters. examples: \\(C_{\\text{gen}}\\)  = C sub gen.  you always reply with the translation nothing else!!!\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Read this latex formula in plain english :  {latex_formula}\"},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Translated LaTeX: {latex_formula} to English: {response['choices'][0]['message']['content']}\")\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def latex_to_english(latex_formulas, debug=False):\n",
    "    \"\"\"\n",
    "    Translate a list of LaTeX formulas to plain English using OpenAI API.\n",
    "\n",
    "    :param latex_formulas: list, a list of LaTeX formulas\n",
    "    :param debug: bool, if True, print debug information\n",
    "    :return: dict, a dictionary where key is the LaTeX formula and value is the translated English text\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        logging.basicConfig(level=logging.DEBUG)\n",
    "    else:\n",
    "        pass\n",
    "        #logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    translated_formulas = {}\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_formula = {executor.submit(translate, formula): formula for formula in latex_formulas}\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_formula):\n",
    "            formula = future_to_formula[future]\n",
    "            try:\n",
    "                translation = future.result()\n",
    "                translated_formulas[formula] = translation\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"{formula} generated an exception: {exc}\")\n",
    "\n",
    "    return translated_formulas\n",
    "\n",
    "\n",
    "\n",
    "def process_markdown_file(file_path, dry_run=False, debug=False, tag=\"_processed\"):\n",
    "    \"\"\"\n",
    "    Process a markdown file, translating LaTeX formulas to plain English.\n",
    "\n",
    "    :param file_path: str, path to the markdown file\n",
    "    :param dry_run: bool, if True, don't write changes back to the file\n",
    "    :param debug: bool, if True, print debug information\n",
    "    :param tag: str, tag to append to the filename of the processed file\n",
    "    \"\"\"\n",
    "    # Dictionary to store LaTeX formulas\n",
    "    latex_formulas = {}\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        matches = re.findall(r\"\\\\\\[(.*?)\\\\\\]|\\\\\\((.*?)\\\\\\)\", line)  # find both inline and display LaTeX formulas\n",
    "        for match in matches:\n",
    "            match = [x for x in match if x]  # remove empty string \n",
    "            if match:\n",
    "                formula = match[0]  # result is in a list\n",
    "                if debug:\n",
    "                    print(f\"Found LaTeX formula: {formula}\")\n",
    "                if formula not in latex_formulas:\n",
    "                    latex_formulas[formula] = None  # Add new formula to the dictionary\n",
    "\n",
    "    print(f\"We found {len(latex_formulas)} formulas in your document\")\n",
    "\n",
    "    # Translate all LaTeX formulas\n",
    "    latex_formulas = latex_to_english(list(latex_formulas.keys()), debug)\n",
    "\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        for formula, translation in latex_formulas.items():\n",
    "            # Replace the formula with its translation\n",
    "            line = line.replace(f\"\\\\[{formula}\\\\]\", translation)\n",
    "            line = line.replace(f\"\\\\({formula}\\\\)\", translation)\n",
    "\n",
    "        new_lines.append(line)\n",
    "\n",
    "    if not dry_run:\n",
    "        base, ext = os.path.splitext(file_path)\n",
    "        new_file_path = f\"{base}{tag}{ext}\"\n",
    "        with open(new_file_path, \"w\") as f:\n",
    "            f.writelines(new_lines)\n",
    "\n",
    "\n",
    "\n",
    "def check_formulas_in_markdown(file_path):\n",
    "    \"\"\"\n",
    "    Check a markdown file, printing out all LaTeX formulas found.\n",
    "\n",
    "    :param file_path: str, path to the markdown file\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        matches = re.findall(r\"\\\\\\((.*?)\\\\\\)\", line)  # find LaTeX formulas\n",
    "        for match in matches:\n",
    "            print(f\"Found LaTeX formula: {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_markdown_file('/home/iwe30/Github/paper2speech/out2/test.md', dry_run=False, debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd vits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "markdown_to_wav('/home/iwe30/Github/paper2speech/out2/test_processed.md', '/home/iwe30/Github/paper2speech/out2/output.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
